import torch
import matplotlib.pyplot as plt
import numpy as np
from transformers import BertTokenizer, BertModel
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler
from mpl_toolkits.mplot3d import Axes3D

tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)

sentences = [""]
target = [""]

def get_embeddings(sentences, word):
    initial_embeddings = []
    contextual_embeddings = []

    for sentence in sentences:
        inputs = tokenizer(sentence, return_tensors='pt', add_special_tokens=True)
        tokens = tokenizer.convert_ids_to_tokens(inputs["input_ids"][0])

        # Identify the indexes of target tokens
        word_indexes = []
        for i, token in enumerate(tokens):
            # Check if the token is a wordpart
            if token == word or (token.startswith('##') and tokens[i-1] == word):
                word_indexes.append(i)
        with torch.no_grad():
            outputs = model(**inputs)
            hidden_states = outputs.hidden_states

        # Get initial embeddings
        initial_embedding = hidden_states[0].squeeze(0)[word_indexes]
        initial_embeddings.append(initial_embedding.mean(dim=0).numpy())

        # Get contextual embeddings
        final_embedding = hidden_states[-1].squeeze(0)[word_indexes]
        contextual_embeddings.append(final_embedding.mean(dim=0).numpy())

    return np.array(initial_embeddings), np.array(contextual_embeddings)

initial_embeddings_target, contextual_embeddings_target = get_embeddings(sentences, target)

def plot_embeddings(initial_embeddings_target, contextual_embeddings_target, title):
    combined_embeddings = np.vstack((initial_embeddings_target, contextual_embeddings_target))
    scaler = StandardScaler()
    scaled_embeddings = scaler.fit_transform(combined_embeddings)

    pca = PCA(n_components=3)
    reduced_embeddings = pca.fit_transform(scaled_embeddings)

    len_target = len(initial_embeddings_target)
    reduced_initial_target = reduced_embeddings[:len_target]
    reduced_contextual_target = reduced_embeddings[len_target:]

    fig = plt.figure()
    ax = fig.add_subplot(111, projection='3d')
    ax.scatter(reduced_initial_target[:,0], reduced_initial_target[:,1], reduced_initial_target[:,2], color='darkgreen', marker='x')
    ax.scatter(reduced_contextual_target[:,0], reduced_contextual_target[:,1], reduced_contextual_target[:,2], color='limegreen', marker='^')

    ax.set_title(title)
    plt.legend(fontsize="small")
    ax.set_xticklabels([])
    ax.set_yticklabels([])
    ax.set_zticklabels([])

    plt.show()
