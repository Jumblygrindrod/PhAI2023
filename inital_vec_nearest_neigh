!pip install transformers
!pip install torch
!pip install matplotlib
!pip install scikit-learn

from transformers import BertTokenizer, BertModel
import torch
from scipy.spatial.distance import cosine


tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')
model = BertModel.from_pretrained('bert-base-uncased', output_hidden_states=True)
model.eval()

def read_words(file_path):
    with open(file_path, 'r') as file:
        return [line.strip() for line in file.readlines()]

def get_initial_embedding(word):
    encoded_input = tokenizer(word, return_tensors='pt')
    with torch.no_grad():
        outputs = model(**encoded_input)
    hidden_states = outputs.hidden_states[0]
    token_embeddings = hidden_states[0, 1:-1, :]
    aggregated_embedding = torch.mean(token_embeddings, dim=0)
    return aggregated_embedding


# 10,000 most common english words taken from: https://github.com/first20hours/google-10000-english/blob/master/google-10000-english.txt 
words = read_words('/content/google-10000-english.txt')


embeddings = {word: get_initial_embedding(word) for word in words}


def get_most_similar_words(target_word, embeddings, top_n):
    if target_word not in embeddings:
        raise ValueError(f"The word '{target_word}' is not in the embeddings.")

    target_embedding = embeddings[target_word]

    similarities = {}
    for word, embedding in embeddings.items():
        if word != target_word:
            similarity = 1 - cosine(target_embedding, embedding)
            similarities[word] = similarity
    
    sorted_similarities = sorted(similarities.items(), key=lambda item: item[1], reverse=True)

    return sorted_similarities[:top_n]

